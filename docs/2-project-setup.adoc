
:imagesdir: img
:source-highlighter: coderay
:icons: font

== Project Setup

Setting up the project should be pretty straight forward.
If you feel any struggle, you can always checkout to the `solution` branch where you will find the `code` directory in the file tree, but keep in mind that learning through error
is the best approach to get the most out of this workshop, so don't give into the frustration!

=== Setting up the SpringBoot application

Provisioning a SpringBoot app, should be a fairly easy task. You can just simply click https://start.spring.io/#!type=gradle-project&language=java&platformVersion=3.5.5&packaging=jar&jvmVersion=17&groupId=com.contoso&artifactId=adviser&name=adviser&description=Nutrition%20adviser&packageName=com.contoso.adviser&dependencies=spring-ai-ollama,data-jpa,web,actuator,devtools,postgresql,lombok[this link] or visit https://start.spring.io
and configure your application from there.

If you choose the latter, make sure to set up your application like this:

image::spring-initializer.png[align=center]

Once your set-up is complete, download the archive extract it and open it in your IDE.
Most popular IDEs will instantly recognize it's a Gradle/Maven project and will set everything up for you.

[TIP]
====
In case you don't want to run your project through an IDE or have any issues with setting up gradle, you can always use the built-in `gradlew` script, coming with your packed project.

To run the application from the terminal, you can execute

[source, bash]
----
#assuming ./ is the root of your extracted project.
./gradlew boot run
----
====

> But hold your horses! We're not done setting up the project yet.

=== The Docker set-up

The first step of making our project is ensuring that all the supporting structure is up and running.
For that you will need to have docker installed on your machine.
We will create three files, that will orchestrate the rest.

==== docker-compose.yml

Inside your project directory, create a file, named `docker-compose.yml`.
The contents of this file should be as follows:

[source, yaml]
----
version: '3'
volumes:
  database:
  cache:

services:

  postgres:
    image: postgres:14.2
    restart: always
    volumes: <.>
      - ./docker/database/data:/var/lib/postgresql/data
      - ./docker/database/postgres:/docker-entrypoint-initdb.d
    ports:
      - "5432:5432"
    environment: <.>
      POSTGRES_PASSWORD: adviser
      POSTGRES_USER: adviser
      DATABASES: adviser

  ollama:
    build:
      context: .  <.>
      dockerfile: ./Dockerfile.ollama <.>
    image: ollama
    container_name: ollama
    entrypoint: /tmp/run-ollama.sh <.>
    environment:
      - OLLAMA_HOST=0.0.0.0
    ports:
      - 11434:11434
    volumes: <.>
      - ./docker/ollama/app:/app/
      - ./docker/ollama/ollama:/root/.ollama
    tty: true
    restart: always
#Uncomment to enable GPU capabilities (https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html)
#    deploy:
#      resources:
#        reservations:
#          devices:
#            - driver: nvidia
#              count: 1
#              capabilities: [ gpu ]

----
<.> We choose to place the database content in a visible volume folder within the project.
This will help you manage and move your project files easily between environments, without having to wonder where they went.

<.> For ease of use, we pre-configure a database, called `adviser` with a user `adviser` and password `adviser`.

<.> Determines the root of your docker configuration.
Make sure that you place your docker configuration files in the path specified by this parameter.

<.> For Ollama we will use a special `Dockerfile`, in order to tell the container to download the LLM automatically.
This is rather a workaround, as the official Ollama containers do not support this out of the box.

<.> Additionally, we will need a special script, which is going to be run within the Ollama container to initiate Ollama
and tell it to pull the required image if missing.

<.> The files pulled by Ollama will also be stored in their own folder, within the project directory.

TIP: If you feel more adventurous, you can try running Ollama on your graphics card.
This will improve the LLM response time significantly, although setting up this feature, requires you to install https://developer.nvidia.com/cuda-toolkit[Nvidia CUDA
drivers], and https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html[Nvidia pass-through for Docker] containers.
These steps are not covered by this workshop, so please refer to the respective tutorials, if you want to delve in to that any further.

==== The Dockerfile

Once we have `docker-compose.yml`, we can proceed creating the docker file within the same folder.
Create a file, named `Dockerfile.ollama`, with the following contents:

[source, dockerfile]
----
FROM ollama/ollama

COPY ./run-ollama.sh /tmp/run-ollama.sh <.>

WORKDIR /tmp

RUN chmod +x run-ollama.sh

EXPOSE 11434 <.>

----
<.> The left hand of the `COPY` command is the place where the `run-ollama.sh` file is located, the right hand is
where the file will be copied inside the container.

<.> With the `EXPOSE` command we tell Docker to open port `11434`, which is the default port to access the Ollama API.
(Make sure it's not used! ðŸ˜œ)

==== `run-ollama.sh`

One final file for this part - the `run-ollama.sh` file.
Again create it in the same folder, and add the following contents:

[source, bash]
----
#!/bin/bash

echo "Starting Ollama server..."
ollama serve &
ollama run llama3.2:3b <.>


echo "Waiting for Ollama server to be active..."
while [ "$(ollama list | grep 'NAME')" == "" ]; do
  sleep 1
done
----
<.> This is the place, where we tell Ollama, which model to install.
You can go with the one given here, or choose your own favorite flavour of LLM, just keep in mind that each LLM responds differently,
and you might get different results or processing times if you go with a different LLM.

Now that we have all the files created let's open a terminal into the containing folder and run `docker-compose up`.

[WARNING]
====
Thi installation will take same time, so do be patient!
It will first download all the required docker images, and then will attempt to pull your LLM model.
The whole set-up will take around 4GB of your hard drive.
====

Once everything is completed you should see the following text in your terminal

image::docker-ready.png[align=center]

Now let's make sure all the components are working...

. For the database, you can try to connect directly from your IDE's database plugin.
You will need that to access your data manually anyway.
If you entered everything correctly you should be able to connect to it.
+
image::database-ready.png[align=center]

. To check if Ollama is working, simply make a GET request to `localhost:11434`
+
[source, bash]
----
curl -v localhost:11434
----
+
As a response you should get
+
[source, text]
----
* Host localhost:11434 was resolved.
* IPv6: ::1
* IPv4: 127.0.0.1
*   Trying [::1]:11434...
* Connected to localhost (::1) port 11434
> GET / HTTP/1.1
> Host: localhost:11434
> User-Agent: curl/8.5.0
> Accept: */*
>
< HTTP/1.1 200 OK
< Content-Type: text/plain; charset=utf-8
< Date: Thu, 03 Apr 2025 17:47:26 GMT
< Content-Length: 17
<
* Connection #0 to host localhost left intact
Ollama is running
----

If that is all set, we can move tho the next step.

==== Connecting Ollama and Database with the application

Before firing up our application for the first time, we need to provide it with the configuration it depends on.
To do so, we will need to modify the `application.properties` file, located under `src/main/resources`:

[source, properties]
----
spring.application.name=adviser
spring.datasource.driver-class-name=org.postgresql.Driver
spring.datasource.username=adviser
spring.datasource.password=adviser
spring.jpa.properties.hibernate.dialect=org.hibernate.dialect.PostgreSQLDialect
spring.datasource.url=jdbc:postgresql://localhost:5432/adviser
spring.jpa.hibernate.ddl-auto=update <.>

spring.ai.ollama.base-url=http://localhost:11434
spring.ai.ollama.chat.options.model=llama3.2:3b <.>
----
<.> For the newbies here, this setting will help us generate all the database tables automatically and keep it up to date with any
future changes.
<.> The name of the model should mirror exactly what is written in `run-ollama.sh`.

Now you can run the application to see if it boots up correctly.

==== Creating our first endpoint

To make sure that all of our components are configured correctly, we will create the base of our application.
To start off, make sure to follow the same project structure.
It will make it easier to navigate throughout this workshop.

image::project-structure.png[align=center]

. In the controller folder create a class called `HelloController.java`.
This will be our welcoming point that checks if everything is up and running.
+
[source, java]
----
package com.contoso.adviser.controller;

import lombok.AllArgsConstructor;
import org.springframework.web.bind.annotation.GetMapping;
import org.springframework.web.bind.annotation.RequestMapping;
import org.springframework.web.bind.annotation.RestController;

@RestController
@AllArgsConstructor
@RequestMapping("/hello")
public class HelloController {

    @GetMapping
    public String sayHello() {
        return "Hello!";
    }

}
----
+
Now if you visit http://localhost:8080/hello, you should get a `Hello!` response.
+
Hurry! ðŸŽ‰ðŸ¥³
+
This means that your application is up and running.

. The next step is to add our first database entity.
Create a class called `User` within the `model` folder:
+
[source, java]
----
package com.contoso.adviser.model;

import jakarta.persistence.*;
import lombok.AllArgsConstructor;
import lombok.Getter;
import lombok.NoArgsConstructor;
import lombok.Setter;

@Getter <.>
@Setter
@NoArgsConstructor
@Entity(name = "users")
public class User {

    @Id <.>
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private long id;

    @Version
    private long version;

    public User(String firstName) {
        this.firstName = firstName;
    }

    private String firstName; <.>
}

----
<.> For ease of use we are going to use Project Lombok, to skip some boilerplateing, making getters and setters
<.> `id` and `version` are properties automatically managed by Hibernate.
They are crucial for every database entity we create.
<.> This is the first column we are going to see in the database.
Further on, we will add more properties to this entity.

. The next step will be to crete the `UserRepository` class under the `repository` package.
This will allow us to access the data related to the User entity:
+
[source, java]
----
package com.contoso.adviser.repository;

import com.contoso.adviser.model.User;
import org.springframework.data.repository.CrudRepository;
import org.springframework.stereotype.Repository;

@Repository
public interface UserRepository extends CrudRepository<User, Long> {
}
----

. Now we need to go back to our `HelloController` and modify our endpoint to accept names.
+
[source, jave]
----
@RestController
@AllArgsConstructor <.>
@RequestMapping("/hello")
public class HelloController {

    private final UserRepository userRepository; <.>

    @GetMapping
    public String sayHello(@RequestParam String name) {
        User user = new User(name);
        userRepository.save(user);
        return "Hello, %s".formatted(user.getFirstName());
    }

}
----
<.> We add `@AllArgsConstructor` annotation to ensure the injection of `UserRepository` inside the controller.
<.> Injecting `UserRepository` is as simple as defining it.
+
Now when we call http://localhost:8080/hello?name=John, for example, we should get `Hello, John` in the response, and have
John persisted in the database. Another module completed!

. Time to set up the communication with Ollama.
Inside the `ai` package, create a service class, called `OllamaAIService`.
This class will serve as a facade, when we want to call Ollama and the LLM:
+
[source, java]
----
package com.contoso.adviser.ai;

import lombok.AllArgsConstructor;
import org.springframework.ai.chat.messages.SystemMessage;
import org.springframework.ai.chat.messages.UserMessage;
import org.springframework.ai.chat.prompt.Prompt;
import org.springframework.ai.ollama.OllamaChatModel;
import org.springframework.stereotype.Service;

import java.util.List;

@Service
@AllArgsConstructor
public class OllamaAiService {

    private static final String PROMPT_GENERAL_INSTRUCTIONS = """ <.>
        Here are the general guidelines to answer the `user_main_prompt`

        """;


    private static final String CURRENT_PROMPT_INSTRUCTIONS = """ <.>

        Here's the `user_main_prompt`:


        """;


    private final OllamaChatModel client;

    public String call(String instructions, String request) {
        var generalInstructions = new SystemMessage(PROMPT_GENERAL_INSTRUCTIONS.concat(instructions));
        var promptMessage = new UserMessage(CURRENT_PROMPT_INSTRUCTIONS.concat(request));

        var prompt = new Prompt(List.of(generalInstructions, promptMessage));
        return client.call(prompt).getResult().getOutput().getText();
    }
}
----
<.> Starting the template for the LLM's purpose
<.> Setting the template for the user's prompt.
+
[TIP]
====
LLMs work through text communication. A user prompt contains manly two parts:

* *System prompt* - it determines the purpose of the LLM.
A LLM is intended to have knowledge in many fields.
Due to this reason it has to understand different contexts.
A drawback of that is that when it's asked to do a certain task, it might not get it correctly, or it might not give us the expected response.
The solution is to assign a role to the LLM, define what our input is going to be and what output we expect.
This will help it better act as the thin we want it to be.

* *User prompt* - a reflection of the system prompt is the user prompt.
Once the LLM has its role assigned, all the input from the user will be treated in the way it is described inside the system prompt.
We will see how this all comes together very soon.
====

. Now it's time to call the LLM from our `HelloController`.
+
[source, java]
----
public class HelloController {

    ...
    private final OllamaAiService ollamaAiService;

    @GetMapping
    public String sayHello(@RequestParam String name) {
        //create and persist user

        String lastName = ollamaAiService.call(""" <.>
                You are a helpful assistant. You will be given a first name of a person.
                Your purpose is to invent the last name of that person and return it as a repsponse.
                For example:

                User: John
                AI: Smith

                User: Anne
                AI: Croft

                You should return only a string containing a single name.
                """, name);

        return "Hello, %s %s!".formatted(user.getFirstName(), lastName);
    }

}
----
<.> As you can see we give a definite purpose to the LLM. We tell it what the input is going to be, and what we expect as output.
We also give it some samples, so it can better understand the task.
The more detailed your system prompt is, the more accurate the LLM will be in responding.

Completing this step concluded the set-up in this chapter.
Now if you call `http://localhost:8080/hello?name=Jennie` for example, you should see a message like: `Hello, Jennie Reid!`

Are you getting it now?
If so, then you gotta keep reading through the next chapters!
We are going to set up our database correctly and implement our aforementioned use cases to make our project complete.