
:imagesdir: img
:source-highlighter: coderay
:icons: font

== Project Structure

Before we begin, let's take a step back and visualize what we're building.
Think of this as assembling a high-performance team, where each component plays a crucial role.
This section will guide you through how everything fits together.

We'll start with a high-level diagram that paints the big picture.

=== Component Diagram

At its core, our system consists of several key components working together like a well-oiled machine:

.Component Diagram
image::component-diagram.svg[align=center]

* *Spring Boot API Server* - The central hub of our web application service.
This is where user interactions happen, prompts are processed, and responses from the LLM are handled.

* *Ollama API Server* - Acting as the bridge to the LLM, this component makes AI integration straightforward.
With Spring Boot's powerful plugins, we can connect it with minimal configuration and call its methods efficiently.

* *LLM Model* - The brain of our intelligent web application.
This is where the real magic happens.
It processes user input, analyzes context, and crafts meaningful responses based on its training data.

* *PostgreSQL Database* - Our knowledge repository.
It stores user input, processed data, and LLM responses, helping us avoid redundant queries and making the system more
efficient over time.

=== High-Level Overview

To bring this structure to life, let's walk through a real-world example.
Imagine a user interacting with our AI-powered nutrition assistant.
How does their request travel through the system?
Let's have look at some use cases that will show you the big picture, shall we?

==== Registering information

Imagine using the app for the first time‚Äîfresh start, no data.
That means we‚Äôll need to query the LLM frequently at the beginning.
But as time passes, the app becomes smarter.
When a user repeats requests, the database will already contain previous LLM responses, reducing unnecessary queries and
making the system faster over time.

image::user-creates-a-meal.svg[align=center]

. The user creates an account, providing essential details like name, age, gender, height, weight, and recommended daily caloric intake.

. The API processes this information and stores it in the database.

. The system confirms that the data has been saved, allowing the user to start adding meal information.

. The user logs a meal by calling an API endpoint (e.g., [5xüçÖ, 1xüçï, 3xü•ö]).

. The server processes and saves the meal data.

. A confirmation is sent back to the user.

. The system checks if any foods are missing detailed nutritional information, such as calories, nutrients, and allergens.

. If necessary, the server builds a custom prompt for the LLM, requesting details for new food items.

. Ollama receives the prompt and sends it to the LLM.

. The LLM works its magic, analyzing the data and generating a response.

. The response is passed back to Ollama.

. The formatted response is returned to the Spring Boot server.

. The server extracts and validates the information.
Any errors or inconsistencies are handled to prevent storing inaccurate data.

. Once validated, the enriched data is saved in the database for future use, reducing the need for repeated LLM queries.

[NOTE]
====
Since LLM queries take time, we delegate these tasks to an asynchronous thread.
This ensures the user doesn't have to wait for a response while the system processes the data in the background.
====

==== Requesting food information

Sometimes, users may want to check the nutritional details of specific foods before consuming them.
Our system will allow users to retrieve this data on demand, ensuring they make informed choices.

image::user-requests-food-details.svg[align=center]

. The user queries the app for food details (e.g., [üçÖ, üçÜ, üçä]).

. The server checks whether the requested food items already exist in the database.

. If the data exists, the system formats it and returns it to the user,
and if any information is missing, the user is notified that data is being generated and should check back later.

. Meanwhile, the server builds a structured prompt for the LLM to request missing food details.

. The LLM processes the request and returns the necessary nutritional data.

. The response is validated.

. The data parsed, and stored in the database.

Next time the user requests the same food, the data will be available instantly‚Äîno need to wait for LLM processing!

==== Asking for meal review

Now that users can log meals, they might wonder:
*How healthy was my last meal?*
This is where the LLM steps in as a virtual nutritionist, analyzing the user‚Äôs meal and providing personalized feedback.

image::user-requests-meal-review.svg[align=center]

. The user enters their meal details ([6xüçå, 1xü•î, 0.5xü´ö]).

. The server saves this data into the relevant database entities.

. The user gets an immediate confirmation.

. Behind the scenes, the system retrieves the meal data and relevant user attributes (age, gender, weight, height).

. A detailed prompt is generated for the LLM, asking it to analyze the meal and provide a review based on the user‚Äôs profile.

. The LLM processes the request.

. Ollama returns an insightful meal review.

. The response is stored in the database alongside the meal entry.

. The user later requests a review for their meal.

. The system retrieves the analysis along with the meal data.

. A response is returned to the user.
If the review is still being processed, the system informs the user to check back soon.

[TIP]
====
These are just a few examples of how an LLM can enhance an application like this.
But the possibilities don‚Äôt stop here!
You can take what you‚Äôve learned and expand this project with new features and use cases.
Get creative!
====

In the next chapter we are going to start setting up our project, so we are ready to dig in and make each of these use cases
come to life.