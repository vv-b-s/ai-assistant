
:imagesdir: img
:source-highlighter: coderay
:icons: font

== Project Setup

Setting up the project should be pretty straight forward.
If you feel any struggle, you can always look at the link:../code[code] directory, but keep in mind that learning through error
is the best approach to get the most out of this workshop, so don't give into the frustration!

=== The Docker set-up

The first step of making our project is ensuring that all the supporting structure is up and running.
For that you will need to have docker installed on your machine.
We will create three files, that will orchestrate the rest.

==== docker-compose.yml

Inside your project directory, create a file, named `docker-compose.yml`.
The contents of this file should be as follows:

[source, yaml]
----
version: '3'
volumes:
  database:
  cache:

services:

  postgres:
    image: postgres:14.2
    restart: always
    volumes: <.>
      - ./docker/database/data:/var/lib/postgresql/data
      - ./docker/database/postgres:/docker-entrypoint-initdb.d
    ports:
      - "5432:5432"
    environment: <.>
      POSTGRES_PASSWORD: adviser
      POSTGRES_USER: adviser
      DATABASES: adviser

  ollama:
    build:
      context: .  <.>
      dockerfile: ./Dockerfile.ollama <.>
    image: ollama
    container_name: ollama
    entrypoint: /tmp/run-ollama.sh <.>
    environment:
      - OLLAMA_HOST=0.0.0.0
    ports:
      - 11434:11434
    volumes: <.>
      - ./docker/ollama/app:/app/
      - ./docker/ollama/ollama:/root/.ollama
    tty: true
    restart: always
#Uncomment to enable GPU capabilities (https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html)
#    deploy:
#      resources:
#        reservations:
#          devices:
#            - driver: nvidia
#              count: 1
#              capabilities: [ gpu ]

----
<.> We choose to place the database content in a visible volume folder within the project.
This will help you manage and move your project files easily between environments, without having to wonder where they went.

<.> For ease of use, we pre-configure a database, called `adviser` with a user `adviser` and password `adviser`.

<.> Determines the root of your docker configuration.
Make sure that you place your docker configuration files in the path specified by this parameter.

<.> For Ollama we will use a special `Dockerfile`, in order to tell the container to download the LLM automatically.
This is rather a workaround, as the official Ollama containers do not support this out of the box.

<.> Additionally, we will need a special script, which is going to be run within the Ollama container to initiate Ollama
and tell it to pull the required image if missing.

<.> The files pulled by Ollama will also be stored in their own folder, within the project directory.

TIP: If you feel more adventurous, you can try running Ollama on your graphics card.
This will improve the LLM response time significantly, although setting up this feature, requires you to install https://developer.nvidia.com/cuda-toolkit[Nvidia CUDA
drivers], and https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html[Nvidia pass-through for Docker] containers.
These steps are not covered by this workshop, so please refer to the respective tutorials, if you want to delve in to that any further.

==== The Dockerfile

Once we have `docker-compose.yml`, we can proceed creating the docker file within the same folder.
Create a file, named `Dockerfile.ollama`, with the following contents:

[source, dockerfile]
----
FROM ollama/ollama

COPY ./run-ollama.sh /tmp/run-ollama.sh <.>

WORKDIR /tmp

RUN chmod +x run-ollama.sh

EXPOSE 11434 <.>

----
<.> The left hand of the `COPY` command is the place where the `run-ollama.sh` file is located, the right hand is
where the file will be copied inside the container.

<.> With the `EXPOSE` command we tell Docker to open port `11434`, which is the default port to access the Ollama API.
(Make sure it's not used! ðŸ˜œ)

==== `run-ollama.sh`

One final file for this part - the `run-ollama.sh` file.
Again create it in the same folder, and add the following contents:

[source, bash]
----
#!/bin/bash

echo "Starting Ollama server..."
ollama serve &
ollama run llama3.2:3b <.>


echo "Waiting for Ollama server to be active..."
while [ "$(ollama list | grep 'NAME')" == "" ]; do
  sleep 1
done
----
<.> This is the place, where we tell Ollama, which model to install.
You can go with the one given here, or choose your own favorite flavour of LLM, just keep in mind that each LLM responds differently,
and you might get different results or processing times if you go with a different LLM.

Now that we have all the files created let's open a terminal into the containing folder and run `docker-compose up`.

[WARNING]
====
Thi installation will take same time, so do be patient!
It will first download all the required docker images, and then will attempt to pull your LLM model.
The whole set-up will take around 4GB of your hard drive.
====

Once everything is completed you should see the following text in your terminal

image::docker-ready.png[align=center]

Now let's make sure all the components are working...

. For the database, you can try to connect directly from your IDE's database plugin.
You will need that to access your data manually anyway.
If you entered everything correctly you should be able to connect to it.
+
image::database-ready.png[align=center]

. To check if Ollama is working, simply make a GET request to `localhost:11434`
+
[source, bash]
----
curl -v localhost:11434
----
+
As a response you should get
+
[source, text]
----
* Host localhost:11434 was resolved.
* IPv6: ::1
* IPv4: 127.0.0.1
*   Trying [::1]:11434...
* Connected to localhost (::1) port 11434
> GET / HTTP/1.1
> Host: localhost:11434
> User-Agent: curl/8.5.0
> Accept: */*
>
< HTTP/1.1 200 OK
< Content-Type: text/plain; charset=utf-8
< Date: Thu, 03 Apr 2025 17:47:26 GMT
< Content-Length: 17
<
* Connection #0 to host localhost left intact
Ollama is running
----

If that is all set, we can move tho the next step.

=== Setting up the SpringBoot application

Provisioning a SpringBoot app, should be a fairly easy task. You can just simply visit the https://start.spring.io/[Spring Initializer]
and configure your application from there.
As a starting point, make sure to set up your application like this:

image::spring-initializer.png[align=center]