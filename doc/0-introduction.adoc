:imagesdir: img
:source-highlighter: coderay
:icons: font

== Introduction

=== And so we meet. Let's get to know each other...

Hey there!
You have probably read the title and the description of this workshop, but to make sure we are on the same page,
I'll try to answer some questions in this chapter.

.What is this workshop for?
****
Obviously, building an AI assistant...

Okay, this is not a good explanation. Let's break it down a bit.

The idea of this workshop is to show you how you can utilize the power of Large Language Models (LLMs) in your web
application and make it even more flexible.
There are a plethora of use cases where you can combine LLMs with the business logic of a server application,
with only your imagination as the limitation.

Unlike simple tutorials that show API calls, this workshop will take a practical, hands-on approach.
By the end, you will have a working web application that leverages an LLM for real-world tasks.
****

.How does this suit me?
****
This suits you in many ways.
Have you ever thought of a scenario where your application has a lot of input data that needs a human-like approach to analyze?
Let's look at some examples:

*Example 1:*
 +
You're building an app that tracks people's fitness and eating habits.
The app monitors metrics like the person's bodily parameters (weight, height, age, gender, etc.),
and it also tracks their eating habits, including the foods and meals they consume, caloric intake, nutritional levels, and so on.
Usually, you would need someone to enter the data of each food, determine its nutrient content, and add logic to combine
these parameters in a scientifically proven method that will inform the user about their health choices.

The problem?
 +
You can't manage all the foods in the world and find out all the information that your app needs to provide proper information to the user.
Also, one size does not fit all.
What if you needed to output information that is more targeted to the user?

The solution?
 +
You integrate an LLM into your application.
You add a specific prompt, customized to the user's needs, and ask it to return the appropriate data.
The LLM can be used to look up nutritional data and process health insights tailored to the user's needs.
The data can also be saved to the database to be reused for other users who might have the same requirements.

'''

*Example 2:*
 +
You build librarian software.
Your application keeps track of all the books contained within a library.
It also knows where each book can be found.
Your application searches for the right book, tracks book rentals, and helps users navigate to the right section.

The problem?
 +
What if you wanted your application to do a bit more? For example, give appropriate recommendations to the user based on a description of what they are looking for, or let the app recommend books from your library to that user based on their ratings of previous books?

The solution?
 +
Use an LLM.
You can create specific LLM prompts that include the user's description of the book, their preferences, and ratings of previous readings.
The LLM could be trained on book knowledge, and you can even implement logic where the LLM interacts with your application, asking if this book is in the library or not, so it can build up a list of the books that are only present within your library.

'''

*Example 3:*

You are building a home assistant server capable of managing your smart devices.
The server's API accepts specific intents that trigger hardcoded scenarios to switch on/off lights, collect data from sensors, etc.

The problem?
 +
You want your server to be smarter.
You want your intents to be more flexible.
For example, when you say "Lights on!" to your voice assistant, you want it to turn on certain lights, depending on the time and atmosphere you desire.

The solution?
 +
You can integrate an LLM into the server.
Your server will forward the voice input from a third-party assistant like Google Home or Alexa and prompt your LLM to
identify the intent and query the right devices.
The LLM will then return a proper response that is read by the home assistant and will trigger the smart devices that
it determines need to be activated according to the prompt.
****

[NOTE]
To keep this section brief, I am providing vague examples here.
All of these will have their specific constraints and challenges, but none of them are impossible.
You can think of any task that requires more abstract thinking and assign an LLM to do it for you.

.Okay! I'm convinced! Where do we go from here?
****
To start off, you're in the right place!
In this workshop, we are going to build such a solution by delving deeper into one of the aforementioned examples.
We will build a web application server that will gather input from the user and consult an LLM for the things that we can't or don't want to figure out on our own.
For that purpose, I am going to show you how to implement the first example.

We will build an application that gathers information about the eating habits of a user and uses an LLM to determine nutritional facts about the food they consume.
As a result, we will provide each user with proper information about their eating habits and generate a review based on the items they consumed.
This will help the user make better and healthier food choices that are more in tune with their personal bodily parameters.
****

.What's the technology stack we're going to use?
****
The technology stack is straightforward and requires minimal setup on your side.
Here is what we're going to use:

* *Spring Boot framework* - the core of our application, handling API requests and processing data.
* *PostgreSQL database* - stores user input and processed nutritional data from the LLM.
* *Ollama server* - a locally hosted web server that enables API communication with an LLM model.
* *Llama LLM* - the chosen LLM for this workshop, due to its lightweight models that can run on most modern machines.
* *Docker* - used to simplify setup by providing a Docker Compose script for database and Ollama server management.
****

[NOTE]
We are not doing fine-tuning in this workshop.
Instead, we will focus on prompt engineering and API-based interaction with pre-trained LLMs.

.But isn't running an LLM locally slow? (The system requirements)
****
Running an LLM locally can be resource-intensive, but for the small-scale tasks in this workshop, most modern machines should handle it fine.

**Recommended system requirements:**

* *A computer running macOS, Linux, or Windows* (*with WSL installed!*)
* *At least 16GB of RAM* - LLMs are memory-intensive.
* *A modern CPU* - the more cores, the better.
* *At least 50GB of free disk space* - required for the LLM model, Docker images, and dependencies.
* *A dedicated GPU* - optional, but it significantly improves performance if properly configured.
****

Now that we're all set, let's move to the next chapter, where I will take you through our project design and show you the big picture.